name: ci-lite

permissions:
  contents: read

concurrency:
  group: ci-lite-${{ github.ref }}
  cancel-in-progress: true

on:
  push:
    branches: [ main ]
  pull_request:
  schedule:
    - cron: '15 3 * * 1' # Mondays 03:15 UTC
  workflow_dispatch:

jobs:
  fast-sqlite:
    name: fast-sqlite (ci-lite) - quick sqlite + ruff
    runs-on: ubuntu-latest
    env:
      DATABASE_URL: ${{ secrets.DATABASE_URL }}  # may be empty
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install backend deps
        run: |
          python -m venv .venv
          . .venv/bin/activate
          python -m pip install --upgrade pip
          pip install -r backend/requirements.txt

      - name: Run quick sqlite tests
        run: . .venv/bin/activate && pytest -q backend/tests/test_db_sqlite.py backend/tests/test_db_utils.py -q

      - name: Alembic offline SQL render (smoke)
        env:
          ALEMBIC_CONFIG: backend/alembic.ini
        run: |
          . .venv/bin/activate
          python -m pip install -r backend/requirements.txt || true
          alembic -c "$ALEMBIC_CONFIG" upgrade head --sql | head -n 5

      - name: Wait for Postgres
        if: >
          github.ref == 'refs/heads/main' ||
          contains(join(fromJson(toJson(github.event.pull_request.labels)).*.name, ','), 'needs-pg')
        run: |
          python - <<'PY'
          import os, time, sqlalchemy as sa
          url=os.environ.get('DATABASE_URL','').replace('+asyncpg','+psycopg2')
          for _ in range(30):
              try:
                  sa.create_engine(url).connect().close()
                  print('Postgres ready')
                  break
              except Exception as e:
                  print('waiting...', e)
                  time.sleep(2)
          else:
              raise SystemExit('Postgres not ready')
          PY

      - name: Run Alembic migrations (online)
        if: >
          github.ref == 'refs/heads/main' &&
          env.DATABASE_URL != '' &&
          startsWith(env.DATABASE_URL, 'postgresql')
        env:
          ALEMBIC_CONFIG: backend/alembic.ini
          PYTHONPATH: ${{ github.workspace }}
        run: |
          . .venv/bin/activate
          pip install -r backend/requirements.txt
          pip install -e ./capsule-core
          alembic -c "$ALEMBIC_CONFIG" upgrade head

  drift_check:
    name: Alembic Drift Check
    runs-on: ubuntu-latest
    if: >
      github.ref == 'refs/heads/main' ||
      contains(join(fromJson(toJson(github.event.pull_request.labels)).*.name, ','), 'run-drift-check')
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r backend/requirements.txt
          pip install -e ./capsule-core

      - name: Schema smoke (metadata → SQLite)
        env:
          ALEMBIC_CONFIG: backend/alembic.ini
          SQLALCHEMY_URL: sqlite:///${{ github.workspace }}/.alembic_tmp/drift.sqlite
          PYTHONPATH: ${{ github.workspace }}
        run: |
          rm -rf .alembic_tmp
          mkdir -p .alembic_tmp .alembic_autogen/drift
          python - <<'PY'
          import os
          from sqlalchemy import create_engine
          # import Base from backend package (PYTHONPATH is set)
          from backend.db.base import Base

          db_path = os.path.join(os.getcwd(), '.alembic_tmp', 'drift.sqlite')
          engine = create_engine(f"sqlite:///{db_path}")
          # Create tables according to model metadata — avoid executing migrations
          Base.metadata.create_all(bind=engine)
          print('Created sqlite drift DB:', db_path)
          PY

  drift_check_pg:
    name: Alembic Drift Check (Postgres)
    runs-on: ubuntu-latest
    # Label-gated: only run when PR has label 'run-drift-check-pg'
    if: >
      contains(join(fromJson(toJson(github.event.pull_request.labels)).*.name, ','), 'run-drift-check-pg')
    services:
      postgres:
        # fall back to an image with pgvector preinstalled when the secret DB
        # is not provided or is unreachable. This image is only pulled for
        # label-gated runs (keeps most PRs lightweight).
        image: ankane/pgvector:latest
        env:
          POSTGRES_PASSWORD: postgres
        ports: ['5432:5432']
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 5s
          --health-timeout 2s
          --health-retries 10
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r backend/requirements.txt
          pip install -e ./capsule-core

      - name: Pick effective DATABASE_URL (reachable-or-local)
        id: pickdb
        env:
          SECRET_DB_URL: ${{ secrets.DATABASE_URL }}
          LOCAL_URL: postgresql+psycopg://postgres:postgres@localhost:5432/postgres
        run: |
          set -e
          python - <<'PY'
          import os, sys
          out = open(os.environ["GITHUB_OUTPUT"], "a")
          secret = os.environ.get("SECRET_DB_URL","").strip()
          eff = os.environ.get("LOCAL_URL")
          def write(url, reason):
              print(f"Effective DATABASE_URL = {url} ({reason})")
              out.write(f"db_url={url}\n")
              out.close()
              sys.exit(0)

          if not secret:
              write(eff, "no secret provided")

          url = secret.replace("+asyncpg", "+psycopg").replace("+psycopg2", "+psycopg")
          if "supabase.co" in url and "sslmode=" not in url:
              sep = "&" if "?" in url else "?"
              url = f"{url}{sep}sslmode=require"

          print("Probing secret DATABASE_URL reachability...")
          try:
              import psycopg
              with psycopg.connect(url, connect_timeout=4) as conn:
                  with conn.cursor() as cur:
                      cur.execute("SELECT 1")
              write(url, "secret reachable")
          except Exception as e:
              print(f"Secret DB unreachable: {e!r}")
              write(eff, "fallback to local service")
          PY

      - name: Wait for local Postgres
        if: contains(steps.pickdb.outputs.db_url, 'localhost')
        run: |
          for i in $(seq 1 60); do
            if pg_isready -h localhost -p 5432 -U postgres; then
              echo "Local PG ready"; exit 0
            fi
            echo "Waiting for local PG..."
            sleep 2
          done
          echo "Local PG not ready"; exit 1

      - name: Print effective DB URL (before upgrade)
        env:
          DATABASE_URL: ${{ steps.pickdb.outputs.db_url }}
        run: |
          python - <<'PY'
          import os, re
          url = os.environ.get('DATABASE_URL') or os.environ.get('SQLALCHEMY_URL','')
          redacted = re.sub(r':\/\/([^:@]+):[^@]+@', r'://\1:***@', url)
          print('Effective DB URL:', redacted)
          PY

      - name: Ensure vector extension (best-effort)
        env:
          DATABASE_URL: ${{ steps.pickdb.outputs.db_url }}
        run: |
          # psql wants a libpq-style URL; strip any +driver suffix from the scheme
          PSQL_URL=$(python - <<'PY'
          import os, re
          u = os.environ.get('DATABASE_URL','')
          u = re.sub(r'^(postgresql)(?:\+[^:]+)?://', r'postgresql://', u)
          print(u)
          PY
          )
          echo "Trying to create vector extension on: ${PSQL_URL//:*@/:***@}"
          psql "$PSQL_URL" -c 'CREATE EXTENSION IF NOT EXISTS vector;' || true
      - name: Upgrade to head on Postgres
        env:
          PYTHONPATH: ${{ github.workspace }}
          ALEMBIC_CONFIG: backend/alembic.ini
          DATABASE_URL: ${{ steps.pickdb.outputs.db_url }}
        run: |
          alembic -c "$ALEMBIC_CONFIG" upgrade head

  tests:
    name: tests (apply migrations + run pytest)
    runs-on: ubuntu-latest
    needs: fast-sqlite
    services:
      pg:
        image: ankane/pgvector:latest
        ports: ["5435:5432"]
        env:
          POSTGRES_USER: aicmo
          POSTGRES_PASSWORD: pass
          POSTGRES_DB: aicmo
        options: >-
          --health-cmd="pg_isready -U aicmo -d aicmo"
          --health-interval=5s --health-timeout=5s --health-retries=20
    env:
      DATABASE_URL: postgresql+psycopg2://aicmo:pass@pg:5432/aicmo
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('backend/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps
        run: |
          python -m venv .venv
          . .venv/bin/activate
          python -m pip install --upgrade pip
          pip install -r backend/requirements.txt
          pip install -r backend/requirements-test.txt || true

      - name: Prep DB + migrate
        run: |
          . .venv/bin/activate
          PSQL_URL=$(python - <<'PY'
          import os,re; u=os.environ['DATABASE_URL']; print(re.sub(r'^(postgresql)(?:\+[^:]+)?://','postgresql://',u))
          PY
          )
          # Clean schema each run
          psql "$PSQL_URL" -c "DROP SCHEMA IF EXISTS public CASCADE; CREATE SCHEMA public;"

          # Single-head guard
          alembic -c backend/alembic.ini heads | awk 'END{ if (NR!=1) { print "Multiple Alembic heads!"; exit 1 } }'

          # Apply migrations
          alembic -c backend/alembic.ini upgrade head

          # === Schema asserts (fail fast) ===
          psql "$PSQL_URL" -Atc "SELECT 1 FROM pg_indexes WHERE tablename='site' AND indexname='ux_site_slug';" | grep 1
          psql "$PSQL_URL" -Atc "SELECT indexdef FROM pg_indexes WHERE tablename='page' AND indexdef LIKE '%UNIQUE% (site_id, path)%';" | grep UNIQUE
          psql "$PSQL_URL" -Atc "SELECT table_name FROM information_schema.views WHERE table_schema='public' AND table_name='site_spec';" | grep site_spec

      - name: Drift probe (fail if model != DB)
        env:
          DATABASE_URL: postgresql+psycopg2://aicmo:pass@localhost:5435/aicmo
        run: |
          . .venv/bin/activate
          python - <<'PY'
          import tempfile, pathlib, sys
          from alembic.config import Config
          from alembic import command
          import os
          tmp = pathlib.Path(tempfile.mkdtemp(prefix="alembic_probe_"))
          cfg = Config("backend/alembic.ini")
          cfg.set_main_option("script_location", "backend/alembic")
          cfg.set_main_option("version_locations", str(tmp / "versions"))
          try:
              command.revision(cfg, message="DRIFT_PROBE", autogenerate=True, rev_id=None)
          except SystemExit:
              pass
          vers = tmp / "versions"
          files = list(vers.glob("*.py")) if vers.exists() else []
          if files:
              print("Autogenerate produced diffs → drift detected.")
              for f in files: print(" -", f.name)
              sys.exit(1)
          print("No drift.")
          PY

      - name: Run tests (Postgres + SQLite)
        env:
          DATABASE_URL: postgresql+psycopg2://aicmo:pass@localhost:5435/aicmo
        run: |
          . .venv/bin/activate
          pytest -q

      - name: Print effective DB URL (before autogenerate)
        env:
          SQLALCHEMY_URL: ${{ steps.pickdb.outputs.db_url }}
        run: |
          python - <<'PY'
          import os, re
          url = os.environ.get('DATABASE_URL') or os.environ.get('SQLALCHEMY_URL','')
          redacted = re.sub(r':\/\/([^:@]+):[^@]+@', r'://\1:***@', url)
          print('Effective DB URL for autogenerate:', redacted)
          PY

      - name: Autogenerate & detect drift (PG)
        env:
          ALEMBIC_CONFIG: backend/alembic.ini
          PYTHONPATH: ${{ github.workspace }}
          DATABASE_URL: ${{ env.DATABASE_URL }}
        run: |
          python - <<'PY'
          import pathlib, tempfile, sys, os, re
          from alembic.config import Config
          from alembic import command

          # 1) Use the repo's real env.py
          cfg = Config(os.environ.get("ALEMBIC_CONFIG", "backend/alembic.ini"))
          repo_script = pathlib.Path("backend/alembic")
          env_py = repo_script / "env.py"
          if not env_py.exists():
              print(f"FATAL: missing {env_py}. Check repo path.")
              sys.exit(2)
          cfg.set_main_option("script_location", str(repo_script))

          # 2) Make a temp versions dir; write *only* revisions here
          tmp_root = pathlib.Path(tempfile.mkdtemp(prefix="alembic_drift_pg_"))
          ver_dir = tmp_root / "versions"
          ver_dir.mkdir(parents=True, exist_ok=True)

          # 3) Try an autogenerate revision
          try:
              rev = command.revision(
                  cfg,
                  message="DRIFT_PROBE_PG",
                  autogenerate=True,
                  version_path=str(ver_dir),
              )
          except SystemExit:
              rev = None

          # 4) Inspect results
          files = sorted(ver_dir.glob("*.py"))
          if not files:
              print("No revision file → no drift.")
              sys.exit(0)

          print("Generated revisions in temp:", [p.name for p in files])

          # 5) Optional: destructive op detector (edit tokens if you like)
          code = files[-1].read_text(encoding="utf-8")
          destructive = re.findall(r"\b(drop_table|drop_column|alter_column|rename_column)\b", code, flags=re.I)
          if destructive:
              print("DESTRUCTIVE TOKENS:", destructive)
              sys.exit(1)
          else:
              print("No destructive ops detected.")
          PY

      - name: Upload drift artifact (PG)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: alembic-drift-autogen-pg
          path: .alembic_autogen/drift_pg/**
          if-no-files-found: warn
          retention-days: 7

      - name: Autogenerate into temp dir and detect drift (SQLite)
        continue-on-error: true
        env:
          ALEMBIC_CONFIG: backend/alembic.ini
          PYTHONPATH: ${{ github.workspace }}
        run: |
          python - <<'PY'
          import pathlib, tempfile, sys, os, re
          from alembic.config import Config
          from alembic import command

          # 1) Use the repo's real env.py
          cfg = Config(os.environ.get("ALEMBIC_CONFIG", "backend/alembic.ini"))
          repo_script = pathlib.Path("backend/alembic")
          env_py = repo_script / "env.py"
          if not env_py.exists():
              print(f"FATAL: missing {env_py}. Check repo path.")
              sys.exit(2)
          cfg.set_main_option("script_location", str(repo_script))

          # 2) Make a temp versions dir; write *only* revisions here
          tmp_root = pathlib.Path(tempfile.mkdtemp(prefix="alembic_drift_sqlite_"))
          ver_dir = tmp_root / "versions"
          ver_dir.mkdir(parents=True, exist_ok=True)

          # 3) Try an autogenerate revision
          try:
              rev = command.revision(
                  cfg,
                  message="DRIFT_PROBE_SQLITE",
                  autogenerate=True,
                  version_path=str(ver_dir),
              )
          except SystemExit:
              rev = None

          # 4) Inspect results
          files = sorted(ver_dir.glob("*.py"))
          if not files:
              print("No revision file created → no drift.")
              sys.exit(0)

          print("Generated revisions in temp:", [p.name for p in files])

          # 5) copy to artifact dir for inspection
          outdir = pathlib.Path(".alembic_autogen/drift")
          outdir.mkdir(parents=True, exist_ok=True)
          out = outdir / files[-1].name
          code = files[-1].read_text(encoding="utf-8")
          out.write_text(code)

          # detect any schema tokens (advisory)
          tokens = ("op.create_table","op.alter_column","op.drop_table","op.add_column","op.drop_column","op.create_index","op.drop_index")
          if any(re.search(rf"\b{t}\b", code, flags=re.I) for t in tokens):
              print(f"Schema drift detected → see {out}")
          else:
              print("Autogenerate produced a trivial/no-op file → no drift.")
          PY

      - name: Upload drift artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: alembic-drift-autogen
          path: .alembic_autogen/drift/**
          retention-days: 7
