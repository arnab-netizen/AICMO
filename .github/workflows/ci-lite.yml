name: ci-lite

on:
  push:
    branches: [ main ]
  pull_request:
  workflow_dispatch:

permissions:
  contents: read

concurrency:
  group: ci-lite-${{ github.ref }}
  cancel-in-progress: true

jobs:
  lint-yaml:
    name: Lint YAML
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Quick YAML parse check
        uses: ibiqlik/action-yamllint@v3
        with:
          file_or_dir: .
          format: github
          continue-on-error: true # Keep as true if you want it to be non-blocking

  fast-sqlite:
    name: fast-sqlite (sqlite smoke + advisory autogen)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          # || true is acceptable here for a "fast" advisory job
          pip install -r backend/requirements.txt || true

      - name: Create advisory SQLite drift artifact
        run: |
          mkdir -p .alembic_autogen/drift
          python - <<'PY'
          import pathlib, tempfile, os, sys
          from alembic.config import Config
          from alembic import command

          cfg = Config('backend/alembic.ini')
          # Point to the dir containing env.py
          cfg.set_main_option('script_location', 'backend/alembic')

          tmp = pathlib.Path(tempfile.mkdtemp(prefix='alembic_drift_sqlite_'))
          ver = tmp / 'versions'
          ver.mkdir(parents=True, exist_ok=True)
          try:
              # Direct output revision to temp dir
              command.revision(cfg, message='DRIFT_PROBE_SQLITE', autogenerate=True, version_path=str(ver))
          except SystemExit:
              pass # Ignore alembic's sys.exit(0)

          files = sorted(ver.glob('*.py'))
          if not files:
              print('No sqlite autogen revisions produced.')
              sys.exit(0)

          # Copy results from temp dir to artifact dir
          outdir = pathlib.Path('.alembic_autogen/drift')
          outdir.mkdir(parents=True, exist_ok=True)
          for f in files:
              (outdir / f.name).write_text(f.read_text(encoding='utf-8'))
          print('Wrote sqlite advisory drift files:', [p.name for p in outdir.glob('*.py')])
          PY

      - name: Upload SQLite advisory artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: alembic-drift-autogen
          path: .alembic_autogen/drift/**
          if-no-files-found: warn
          retention-days: 7

  drift_check_pg:
    name: Alembic Drift Check (Postgres)
    runs-on: ubuntu-latest
    # Label-gated: only run when PR has label 'run-drift-check-pg'
    if: >
      contains(join(fromJson(toJson(github.event.pull_request.labels)).*.name, ','), 'run-drift-check-pg')
    services:
      postgres:
        image: ankane/pgvector:latest
        env:
          POSTGRES_PASSWORD: postgres
        ports: ['5432:5432']
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 5s
          --health-timeout 2s
          --health-retries 10
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r backend/requirements.txt
          # Assuming capsule-core is part of your project
          # pip install -e ./capsule-core

      - name: Pick effective DATABASE_URL (reachable-or-local)
        id: pickdb
        env:
          SECRET_DB_URL: ${{ secrets.DATABASE_URL }}
          LOCAL_URL: postgresql+psycopg://postgres:postgres@localhost:5432/postgres
        run: |
          set -e
          python - <<'PY'
          import os, sys
          out = open(os.environ["GITHUB_OUTPUT"], "a")
          secret = os.environ.get("SECRET_DB_URL","").strip()
          eff = os.environ.get("LOCAL_URL")
          def write(url, reason):
              print(f"Effective DATABASE_URL = {url} ({reason})")
              out.write(f"db_url={url}\n")
              out.close()
              sys.exit(0)

          if not secret:
              write(eff, "no secret provided")

          url = secret.replace("+asyncpg", "+psycopg").replace("+psycopg2", "+psycopg")
          if "supabase.co" in url and "sslmode=" not in url:
              sep = "&" if "?" in url else "?"
              url = f"{url}{sep}sslmode=require"

          print("Probing secret DATABASE_URL reachability...")
          try:
              import psycopg
              with psycopg.connect(url, connect_timeout=4) as conn:
                  with conn.cursor() as cur:
                      cur.execute("SELECT 1")
              write(url, "secret reachable")
          except Exception as e:
              print(f"Secret DB unreachable: {e!r}")
              write(eff, "fallback to local service")
          PY

      - name: Wait for local Postgres
        if: contains(steps.pickdb.outputs.db_url, 'localhost')
        run: |
          for i in $(seq 1 60); do
            if pg_isready -h localhost -p 5432 -U postgres; then
              echo "Local PG ready"; exit 0
            fi
            echo "Waiting for local PG..."
            sleep 2
          done
          echo "Local PG not ready"; exit 1

      - name: Ensure vector extension (best-effort)
        env:
          DATABASE_URL: ${{ steps.pickdb.outputs.db_url }}
        run: |
          PSQL_URL=$(python - <<'PY'
          import os, re
          u = os.environ.get('DATABASE_URL','')
          u = re.sub(r'^(postgresql)(?:\+[^:]+)?://', r'postgresql://', u)
          print(u)
          PY
          )
          echo "Trying to create vector extension on: ${PSQL_URL//:*@/:***@}"
          psql "$PSQL_URL" -c 'CREATE EXTENSION IF NOT EXISTS vector;' || true

      - name: Upgrade to head on Postgres
        env:
          PYTHONPATH: ${{ github.workspace }}
          ALEMBIC_CONFIG: backend/alembic.ini
          DATABASE_URL: ${{ steps.pickdb.outputs.db_url }}
        run: |
          alembic -c "$ALEMBIC_CONFIG" upgrade head

      - name: Autogenerate & detect drift (PG)
        env:
          ALEMBIC_CONFIG: backend/alembic.ini
          PYTHONPATH: ${{ github.workspace }}
          DATABASE_URL: ${{ steps.pickdb.outputs.db_url }}
        run: |
          python - <<'PY'
          import pathlib, tempfile, sys, os, re
          from alembic.config import Config
          from alembic import command

          cfg = Config(os.environ.get("ALEMBIC_CONFIG", "backend/alembic.ini"))
          repo_script = pathlib.Path("backend/alembic")
          cfg.set_main_option("script_location", str(repo_script))

          tmp_root = pathlib.Path(tempfile.mkdtemp(prefix="alembic_drift_pg_"))
          ver_dir = tmp_root / "versions"
          ver_dir.mkdir(parents=True, exist_ok=True)

          try:
              rev = command.revision(
                  cfg,
                  message="DRIFT_PROBE_PG",
                  autogenerate=True,
                  version_path=str(ver_dir),
              )
          except SystemExit:
              rev = None

          files = sorted(ver_dir.glob("*.py"))
          if not files:
              print("No revision file → no drift.")
              sys.exit(0)

          print("Generated revisions in temp:", [p.name for p in files])

          # --- FIX ---
          # Copy generated files to the artifact directory
          outdir = pathlib.Path(".alembic_autogen/drift_pg")
          outdir.mkdir(parents=True, exist_ok=True)
          for f in files:
              (outdir / f.name).write_text(f.read_text(encoding="utf-8"))
          print(f"Wrote PG drift files to {outdir}:", [p.name for p in files])
          # --- END FIX ---

          code = files[-1].read_text(encoding="utf-8")
          destructive = re.findall(r"\b(drop_table|drop_column|alter_column|rename_column)\b", code, flags=re.I)
          if destructive:
              print("DESTRUCTIVE TOKENS:", destructive)
              # This job is advisory, but you can fail it if you want
              # sys.exit(1)
          else:
              print("No destructive ops detected.")
          PY

      - name: Scan PG autogen for destructive changes
        if: always()
        run: |
          python tools/scan_autogen_for_drops.py .alembic_autogen/drift_pg || (echo "Destructive tokens detected" && exit 1)

      - name: Upload drift artifact (PG)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: alembic-drift-autogen-pg
          path: .alembic_autogen/drift_pg/**
          if-no-files-found: warn
          retention-days: 7

  tests:
    name: tests (apply migrations + run pytest)
    runs-on: ubuntu-latest
    needs: fast-sqlite
    services:
      pg:
        image: ankane/pgvector:latest
        ports: ["5435:5432"]
        env:
          POSTGRES_USER: aicmo
          POSTGRES_PASSWORD: pass
          POSTGRES_DB: aicmo
        options: >-
          --health-cmd="pg_isready -U aicmo -d aicmo"
          --health-interval=5s --health-timeout=5s --health-retries=20
    env:
      DATABASE_URL: postgresql+psycopg2://aicmo:pass@localhost:5435/aicmo
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('backend/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install deps
        run: |
          python -m venv .venv
          . .venv/bin/activate
          python -m pip install --upgrade pip
          pip install -r backend/requirements.txt
          # --- FIX ---
          # Removed '|| true' to ensure test dependency failures stop the build
          pip install -r backend/requirements-test.txt

      - name: Prep DB + migrate
        run: |
          . .venv/bin/activate
          PSQL_URL=$(python - <<'PY'
          import os,re; u=os.environ['DATABASE_URL']; print(re.sub(r'^(postgresql)(?:\+[^:]+)?://','postgresql://',u))
          PY
          )
          # Clean schema each run
          psql "$PSQL_URL" -c "DROP SCHEMA IF EXISTS public CASCADE; CREATE SCHEMA public;"

          # Single-head guard
          alembic -c backend/alembic.ini heads | awk 'END{ if (NR!=1) { print "Multiple Alembic heads!"; exit 1 } }'

          # Apply migrations
          alembic -c backend/alembic.ini upgrade head

          # === Schema asserts (fail fast) ===
          # (These are optional but good asserts, kept from your example)
          psql "$PSQL_URL" -Atc "SELECT 1 FROM pg_indexes WHERE tablename='site' AND indexname='ux_site_slug';" | grep 1
          psql "$PSQL_URL" -Atc "SELECT indexdef FROM pg_indexes WHERE tablename='page' AND indexdef LIKE '%UNIQUE% (site_id, path)%';" | grep UNIQUE
          psql "$PSQL_URL" -Atc "SELECT table_name FROM information_schema.views WHERE table_schema='public' AND table_name='site_spec';" | grep site_spec

      - name: Alembic visibility check
        working-directory: .
        run: |
          set -e
          . .venv/bin/activate
          echo "CWD: $(pwd)"
          echo "Repo root listing:"
          ls -la
          echo "Alembic files:"
          ls -la backend/alembic || true
          ls -la backend/alembic/versions || true

          python - <<'PY'
          import os, glob
          from alembic.config import Config
          from alembic.script import ScriptDirectory

          cfg = Config("backend/alembic.ini")
          print("alembic.ini found:", os.path.exists("backend/alembic.ini"))
          print("cfg.config_file_name:", cfg.config_file_name)
          print("script_location cfg:", cfg.get_main_option("script_location"))

          versions = glob.glob("backend/alembic/versions/*.py")
          print("versions count:", len(versions))
          for v in sorted(versions)[-5:]:
              print("  -", os.path.basename(v))

          script = ScriptDirectory.from_config(cfg)
          print("heads:", list(script.get_heads()))
          print("bases:", list(script.get_bases()))
          PY

      - name: Drift probe (fail if model != DB)
        run: |
          . .venv/bin/activate
          python - <<'PY'
          import tempfile, pathlib, sys
          from alembic.config import Config
          from alembic import command

          tmp = pathlib.Path(tempfile.mkdtemp(prefix="alembic_probe_"))
          out = tmp / "versions"
          out.mkdir(parents=True, exist_ok=True)

          cfg = Config("backend/alembic.ini")
          try:
            command.revision(
              cfg,
              message="DRIFT_PROBE",
              autogenerate=True,
              version_path=str(out),  # <-- directs output only
            )
          except SystemExit:
            pass # Ignore alembic's sys.exit(0)

          files = list(out.glob("*.py"))
          if files:
            print("Autogenerate produced diffs → drift detected.")
            for f in files:
              print(" -", f.name)
            sys.exit(1) # <-- This is the critical failure
          print("No drift.")
          PY

      - name: Run tests (Postgres + SQLite)
        env:
          DATABASE_URL: postgresql+psycopg2://aicmo:pass@localhost:5435/aicmo
        run: |
          . .venv/bin/activate
          pytest -q

      - name: Alembic visibility check (list heads from real script_location)
        working-directory: .
        run: |
          python - <<'PY'
          import os
          from alembic.config import Config
          from alembic.script import ScriptDirectory

          cfg = Config("backend/alembic.ini")
          script_location = cfg.get_main_option("script_location")
          print(f"[dbg] real script_location={script_location}")

          sd = ScriptDirectory.from_config(cfg)
          heads = list(sd.get_heads())
          print(f"[dbg] alembic heads count={len(heads)} -> {heads}")
          assert len(heads) >= 1, "No heads found — Alembic cannot see versions"
          PY

      - name: Scan drift artifacts for destructive changes
        if: always()
        run: |
          python tools/scan_autogen_for_drops.py .alembic_autogen/drift_pg || (echo "Destructive tokens detected" && exit 1)
