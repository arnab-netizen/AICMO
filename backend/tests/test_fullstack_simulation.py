"""
Fullstack simulation tests for AICMO.

Goal:
- Exercise all major packs and features end-to-end using the real FastAPI app.
- Prove that report generation, benchmarks, PDF export, learning, and review
  responder all work without crashes and produce structurally sane outputs.

This file is intentionally high-level and opinionated:
- If any critical surface breaks, these tests MUST fail loudly.
- Do not weaken assertions without a very clear reason.
"""

from typing import Any, Dict, List

import pytest
from fastapi.testclient import TestClient

from backend.main import app  # FastAPI app
from aicmo.presets.package_presets import PACKAGE_PRESETS


client = TestClient(app)


def _all_pack_keys() -> List[str]:
    """Discover all pack keys from PACKAGE_PRESETS dynamically."""
    keys = []
    for key, preset in PACKAGE_PRESETS.items():
        # Some presets may be aliases or variants â€“ include all with a sections list.
        sections = preset.get("sections") or preset.get("section_ids") or []
        if sections:
            keys.append(key)
    return sorted(set(keys))


def _make_minimal_brand_brief() -> Dict[str, Any]:
    """
    Construct a minimal but valid BrandBrief-like payload.

    IMPORTANT:
    - If your actual BrandBrief schema has additional required fields,
      extend this function rather than editing tests everywhere.
    """
    return {
        "brand_name": "Simulated Test Brand",
        "industry": "Food & Beverage",
        "location": "Indiranagar, Bangalore",
        "primary_goal": "Increase daily footfall and Instagram visibility",
        "secondary_goal": "Build a loyal local community",
        "brand_tone": ["friendly", "casual", "aesthetic"],
        "target_audience": [
            "Students",
            "Remote workers",
            "Young professionals",
        ],
        # Add extra fields here if your Pydantic model requires them:
        # "website": "https://example.com",
        # "budget_level": "medium",
        # "time_horizon": "3 months",
    }


def _extract_sections_from_response(data: Dict[str, Any]) -> List[Dict[str, str]]:
    """
    Normalise whatever structure the generate_report endpoint returns into a
    list of {id, content} for benchmark validation.

    Current API returns: {"report_markdown": "...", "status": "success"}
    We parse the markdown into sections based on ## headings.
    """
    # Actual API response structure
    markdown = data.get("report_markdown") or data.get("markdown") or ""
    if not markdown:
        return []

    # Parse markdown into sections based on ## headings
    import re

    sections = []
    # Split by ## headings (level 2)
    parts = re.split(r"^## ", markdown, flags=re.MULTILINE)

    for i, part in enumerate(parts[1:], 1):  # Skip first empty part
        # Extract section title
        lines = part.split("\n", 1)
        if lines:
            title = lines[0].strip()
            # Create a section_id from the title
            section_id = title.lower().replace(" ", "_").replace("&", "and")
            section_id = re.sub(r"[^a-z0-9_]", "", section_id)
            sections.append({"id": section_id, "content": f"## {part}"})

    return sections


# ------------- 1. CORE PACK SIMULATION -------------------------------------


@pytest.mark.parametrize("pack_key", _all_pack_keys())
def test_generate_report_for_every_pack_and_validate_benchmarks(pack_key: str):
    """
    Fullstack simulation:
    - Call the real /api/aicmo/generate_report endpoint for every pack.
    - Ensure 200 OK and non-empty markdown report.
    - Validate basic report structure and content.

    Note: Benchmark validation happens internally in generate_sections().
    This test validates the public API works end-to-end.
    """

    brief = _make_minimal_brand_brief()

    payload = {
        "pack_key": pack_key,
        "brand_brief": brief,
    }

    resp = client.post("/api/aicmo/generate_report", json=payload)
    assert resp.status_code == 200, f"{pack_key}: HTTP {resp.status_code} -> {resp.text}"

    data = resp.json()

    # Validate response structure
    assert "report_markdown" in data, f"{pack_key}: Missing 'report_markdown' in response"
    assert "status" in data, f"{pack_key}: Missing 'status' in response"

    markdown = data["report_markdown"]

    # Basic content validation
    assert markdown, f"{pack_key}: Empty report_markdown"
    assert len(markdown) > 500, f"{pack_key}: Report too short ({len(markdown)} chars)"
    assert "##" in markdown, f"{pack_key}: No section headings (##) found in markdown"

    # Validate report contains key brand information
    brand_name = brief.get("brand_name", "")
    if brand_name and brand_name != "Simulated Test Brand":
        assert brand_name in markdown, f"{pack_key}: Brand name '{brand_name}' not found in report"

    # Validate status
    assert (
        data["status"] == "success"
    ), f"{pack_key}: Expected status='success', got '{data['status']}'"


# ------------- 2. PDF EXPORT SIMULATION ------------------------------------


@pytest.mark.parametrize("pack_key", _all_pack_keys()[:3])  # limit to a few packs for speed
def test_pdf_export_for_selected_packs(pack_key: str):
    """
    End-to-end PDF export smoke test:
    - Generate a report for a pack.
    - Call the PDF export endpoint using the same data.
    - Assert we get a binary PDF-ish response.
    """

    brief = _make_minimal_brand_brief()

    payload = {
        "pack_key": pack_key,
        "brand_brief": brief,
    }

    # First, generate the report.
    resp = client.post("/api/aicmo/generate_report", json=payload)
    assert resp.status_code == 200, f"{pack_key}: HTTP {resp.status_code} -> {resp.text}"
    report_data = resp.json()

    # Now call PDF export. Actual endpoint is /aicmo/export/pdf
    # It expects: {"markdown": "..."} or {"sections": [...], "brief": {...}}
    pdf_payload = {
        "markdown": report_data.get("report_markdown", ""),
        "brief": brief,
    }

    pdf_resp = client.post("/aicmo/export/pdf", json=pdf_payload)
    assert (
        pdf_resp.status_code == 200
    ), f"{pack_key}: PDF export failed with HTTP {pdf_resp.status_code} -> {pdf_resp.text}"

    content_type = pdf_resp.headers.get("content-type", "")
    assert "pdf" in content_type.lower() or pdf_resp.content.startswith(b"%PDF"), (
        f"{pack_key}: Export did not return a PDF-like response. " f"Content-Type={content_type}"
    )
    assert len(pdf_resp.content) > 1_000, (
        f"{pack_key}: PDF response too small to be a real report "
        f"(len={len(pdf_resp.content)} bytes)."
    )


# ------------- 3. LEARNING / PHASE L SIMULATION ---------------------------


@pytest.mark.skip(
    reason="Learning endpoint not yet implemented - waiting for /api/learn/from-report"
)
def test_learning_from_generated_report_round_trip():
    """
    Simulate a simple learning round-trip:
    - Generate a report.
    - Send it to the learning endpoint.
    - Assert 200 OK and some kind of success flag/summary.
    """

    # Use one representative pack, e.g., strategy_campaign_premium if available.
    pack_keys = _all_pack_keys()
    candidate = next((k for k in pack_keys if "strategy" in k), pack_keys[0])
    brief = _make_minimal_brand_brief()

    gen_payload = {
        "pack_key": candidate,
        "brand_brief": brief,
    }
    resp = client.post("/api/aicmo/generate_report", json=gen_payload)
    assert resp.status_code == 200, f"{candidate}: HTTP {resp.status_code} -> {resp.text}"
    report_data = resp.json()

    learn_payload = {
        # Adjust to match your actual learning endpoint schema:
        # e.g., {"source": "generated_report", "report": report_data}
        "source": "generated_report",
        "payload": report_data,
    }

    learn_resp = client.post("/api/learn/from-report", json=learn_payload)
    assert (
        learn_resp.status_code == 200
    ), f"Learning endpoint failed with HTTP {learn_resp.status_code} -> {learn_resp.text}"

    body = learn_resp.json()
    # We don't over-spec meta, just ensure some confirmation flag exists.
    assert body, "Learning endpoint returned empty body."
    # Adjust keys according to your real implementation once known.
    # e.g., assert body.get("status") == "ok"


# ------------- 4. REVIEW RESPONDER SIMULATION -----------------------------


@pytest.mark.skip(
    reason="Review responder endpoint not yet implemented - waiting for /api/aicmo/reviews/respond"
)
def test_review_responder_generates_stable_responses():
    """
    Simulate the review responder feature:
    - Send a few sample reviews (positive/negative).
    - Assert the endpoint responds and returns structured outputs.
    """

    sample_reviews = [
        {
            "id": "r1",
            "rating": 5,
            "text": "Absolutely loved the coffee and staff were very friendly!",
        },
        {"id": "r2", "rating": 2, "text": "Service was slow and my order was mixed up twice."},
    ]

    payload = {
        "brand_name": "Simulated Test Brand",
        "platform": "google",
        "reviews": sample_reviews,
    }

    resp = client.post("/api/aicmo/reviews/respond", json=payload)
    assert (
        resp.status_code == 200
    ), f"Review responder failed with HTTP {resp.status_code} -> {resp.text}"

    data = resp.json()
    # Expect something like a list of responses; adjust as per your schema.
    responses = data.get("responses") or data.get("review_responses")
    assert responses and isinstance(
        responses, list
    ), "Review responder did not return a list of responses."
    assert len(responses) == len(
        sample_reviews
    ), "Number of responses does not match number of reviews."


# ------------- 5. NEGATIVE CONSTRAINTS / DON'TS ENFORCEMENT ---------------


@pytest.mark.skip(reason="Constraints field not yet implemented in generate_report payload")
def test_negative_constraints_are_respected_in_output():
    """
    Smoke test for 'don't' rules / negative constraints:
    - Pass a explicit 'do not mention X' rule.
    - Ensure the generated report does NOT contain that phrase.
    """

    brief = _make_minimal_brand_brief()

    dont_phrase = "cheap discounts"
    constraints = {"donts": [f"Do not mention '{dont_phrase}' anywhere in the report."]}

    payload = {
        "pack_key": "quick_social_basic",
        "brand_brief": brief,
        "constraints": constraints,
    }

    resp = client.post("/api/aicmo/generate_report", json=payload)
    assert resp.status_code == 200, f"HTTP {resp.status_code} -> {resp.text}"
    data = resp.json()

    # Flatten all text content and look for the forbidden phrase.
    sections = _extract_sections_from_response(data)
    combined = "\n".join(s["content"] for s in sections).lower()

    assert dont_phrase.lower() not in combined, (
        f"Negative constraint violated: found forbidden phrase '{dont_phrase}' "
        "in generated report."
    )


# ------------- 6. COMPETITOR / LOCATION RESEARCH --------------------------


@pytest.mark.skip(reason="Enable once competitor/location endpoint schema is finalised.")
def test_competitor_research_by_location_smoke():
    """
    OPTIONAL: Enable this once the competitor/location research endpoint is stable.

    - Call competitor research using URL/pincode/location.
    - Assert it returns a sane structure (list of competitors, key metrics).
    """

    payload = {
        "brand_name": "Simulated Test Brand",
        "location": "700094",
        "industry": "Laundry & Dry Cleaning",
        # Adjust field names to match your real endpoint schema:
        # "pincode": "700094",
        # "max_competitors": 5,
    }

    resp = client.post("/api/aicmo/competitors/by-location", json=payload)
    assert (
        resp.status_code == 200
    ), f"Competitor research failed with HTTP {resp.status_code} -> {resp.text}"

    data = resp.json()
    competitors = data.get("competitors") or data.get("results") or []
    assert competitors and isinstance(
        competitors, list
    ), "Competitor research returned no competitors."
