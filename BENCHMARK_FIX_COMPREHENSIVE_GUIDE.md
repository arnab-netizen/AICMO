# Comprehensive Benchmark Validation Fix Guide

## Overview

This guide documents a systematic 5-step approach for fixing benchmark validation failures in AICMO content generation. The methodology was developed to resolve the `full_30_day_calendar` section failing for the `full_funnel_growth_suite` pack.

### Problem Statement

Production error:
```
Benchmark validation failed for pack 'full_funnel_growth_suite' 
after 2 attempt(s). Failing sections: ['full_30_day_calendar']
```

Root causes identified:
- Word count: 1232 (exceeded 1000 max)
- Genericness score: 0.44 (exceeded 0.35 threshold)
- Contained forbidden phrase: "best practices"

---

## The 5-Step Fix Framework

### Step 1: Make Generator Satisfy Spec with Explicit Enforcement

**Objective**: Rewrite the generator to explicitly satisfy all benchmark requirements.

**Key Principles**:
1. Read the exact benchmark specification for the section
2. Extract all constraints (word count, headings, bullets, format)
3. Rewrite generator with explicit compliance documentation
4. Test output against each constraint programmatically

**Example - for `full_30_day_calendar`:**

Location: `backend/main.py`, function `_gen_full_30_day_calendar()`

Benchmark specification (from `learning/benchmarks/section_benchmarks.full_funnel.json`):
```json
{
  "section_id": "full_30_day_calendar",
  "word_count": {"min": 300, "max": 1000},
  "headings": {"min": 4, "max": 10},
  "required_headings": ["Week 1", "Week 2", "Week 3", "Week 4"],
  "bullets": {"min": 12, "max": 40},
  "format": "markdown_table",
  "max_repetition_ratio": 0.35,
  "max_avg_sentence_length": 28,
  "forbidden_phrases": ["post daily", "figure it out later", "lorem ipsum"]
}
```

**Implementation Pattern**:

1. Add explicit docstring documenting all constraints:
```python
def _gen_full_30_day_calendar(req: GenerateRequest, *, pack_key: str) -> str:
    """
    Generate 30-day content calendar section.
    
    Benchmark Requirements:
    - Word count: 300-1000 (target: ~850)
    - Headings: 4-10 (exactly: Week 1, 2, 3, 4)
    - Bullets: 12-40 (target: 20-25 per week)
    - Format: markdown_table (must contain | characters)
    - Max sentence length: 28 characters
    - Forbidden: "post daily", "figure it out later", "lorem ipsum"
    """
```

2. Structure output explicitly:
   - Main heading (##)
   - Intro paragraph (keep brief, ~100 words)
   - Week 1-4 sections (### Week 1, ### Week 2, etc.)
   - Week 1: Include markdown table with pipe characters
   - Weeks 2-4: Include bullet points
   - Keep sentences short (under 28 chars avg)

3. Avoid verbose language that inflates word count:
   - Remove: "Discovery Phase", "Credibility Building", "Strategic Positioning"
   - Use: "Week 1", "Week 2", direct language

4. Ensure specificity (avoid genericness):
   - Reference brand name, industry, product, goal from `req.brief`
   - Example: "`TechCorp` launches this 30-day calendar to guide `Data Engineering Teams`..."

**Validation**:
```python
# After implementation, verify:
output = _gen_full_30_day_calendar(req, pack_key="full_funnel_growth_suite")
assert 300 <= len(output.split()) <= 1000, "Word count out of bounds"
assert all(f"Week {i}" in output for i in range(1,5)), "Missing required headings"
assert "|" in output, "Missing markdown table"
assert 12 <= output.count("- ") <= 40, "Bullet count out of bounds"
assert all(p not in output.lower() for p in FORBIDDEN), "Contains forbidden phrase"
```

**Result**: Generator now produces 843 words, 6 headings, 23 bullets, passes all checks.

---

### Step 2: Add Detailed Logging with Structured Error Information

**Objective**: Provide clear, actionable error messages when validation fails.

**Key Principles**:
1. Log at the point of validation failure
2. Include context: pack key, section ID, attempt number
3. Summarize validation errors (codes, messages, severity)
4. Use structured logging with `extra={}` parameter for machine parsing

**Implementation Location**: `backend/validators/report_enforcer.py`

**Code Pattern**:
```python
import logging

logger = logging.getLogger(__name__)

# In enforce_benchmarks_with_regen():
if validation_fails:
    logger.error(
        f"Benchmark validation failed for pack {pack_key}",
        extra={
            "pack_key": pack_key,
            "section_id": section_id,
            "attempt": attempt,
            "validation_errors": [
                {
                    "code": issue.code,
                    "message": issue.message,
                    "severity": issue.severity,
                }
                for issue in issues
            ],
        }
    )
```

**Current Implementation** (lines 120-150 in `report_enforcer.py`):
```python
logger.error(f"\n{'='*80}\nDETAILED VALIDATION FAILURES FOR {pack_key}:\n{'='*80}")
for fr in failing:
    sid = getattr(fr, "section_id", "UNKNOWN")
    issues = getattr(fr, "issues", [])
    logger.error(f"\nðŸ“‹ Section: {sid}")
    logger.error(f"   Total issues: {len(issues)}")
    for issue in issues[:10]:
        code = getattr(issue, "code", "?")
        msg = getattr(issue, "message", "?")
        sev = getattr(issue, "severity", "?")
        logger.error(f"   [{sev.upper()}] {code}: {msg}")
```

**Benefits**:
- Clear visibility into what failed and why
- Easy to add structured logging later for metrics collection
- Helps debugging by showing first 10 issues per section

---

### Step 3: Create Compliance Tests

**Objective**: Create automated tests that verify generator output meets all benchmark requirements.

**Implementation Location**: `backend/tests/test_full_30_day_calendar_compliance.py`

**Test Structure**:
```python
class TestFullThirtDayCalendarCompliance:
    """Test suite for full_30_day_calendar generator compliance."""
    
    def test_word_count_within_bounds(self):
        """Verify 300-1000 words."""
    
    def test_contains_required_headings(self):
        """Verify Week 1, Week 2, Week 3, Week 4."""
    
    def test_contains_markdown_table(self):
        """Verify pipe characters present."""
    
    def test_bullet_count_within_bounds(self):
        """Verify 12-40 bullets."""
    
    def test_no_forbidden_phrases(self):
        """Verify no blacklisted terms."""
    
    def test_heading_count_within_bounds(self):
        """Verify 4-10 total headings."""
    
    def test_specific_language_not_generic(self):
        """Verify brand/goal/industry language used."""
    
    def test_no_excessive_repetition(self):
        """Verify repetition ratio <= 0.35."""
```

**Test Execution Results**:
```
=== Generator Compliance Metrics ===
Words: 843 (target 300-1000) - âœ“ PASS
Headings: 6 (target 4-10) - âœ“ PASS
Bullets: 23 (target 12-40) - âœ“ PASS
Has pipes (markdown table): True - âœ“ PASS
Has Week 1-4: True - âœ“ PASS
No "post daily": True - âœ“ PASS
```

**Running Tests**:
```bash
cd /workspaces/AICMO
python -m pytest backend/tests/test_full_30_day_calendar_compliance.py -v
```

---

### Step 4: Optional Draft Mode Relaxation

**Objective**: Allow internal/debug iteration without strict validation failure.

**Key Principles**:
1. Add `draft_mode` parameter to validation enforcer
2. When enabled, skip raising errors but still log warnings
3. Useful for development/experimentation before production deployment

**Implementation Location**: `backend/validators/report_enforcer.py`

**Function Signature**:
```python
def enforce_benchmarks_with_regen(
    *,
    pack_key: str,
    sections: List[Dict[str, str]],
    regenerate_failed_sections: Optional[Callable] = None,
    max_attempts: int = 2,
    fallback_to_original: Optional[Dict[str, str]] = None,
    draft_mode: bool = False,  # NEW
) -> EnforcementOutcome:
```

**Implementation Logic**:
```python
# Draft mode: skip strict validation and return sections as-is
if draft_mode:
    logger.info(f"[DRAFT MODE] Skipping strict benchmark validation for pack '{pack_key}'")
    
    # Still run validation to collect metrics/warnings
    validation = validate_report_sections(pack_key=pack_key, sections=sections)
    
    # Return with draft status (not strictly enforced)
    return EnforcementOutcome(
        status="PASS_WITH_WARNINGS",
        sections=list(sections),
        validation=validation,
    )
```

**Usage Pattern**:
```python
# Production (strict validation)
outcome = enforce_benchmarks_with_regen(
    pack_key="full_funnel_growth_suite",
    sections=sections,
    regenerate_failed_sections=my_regen_callback,
)

# Development/Debug (relaxed)
outcome = enforce_benchmarks_with_regen(
    pack_key="full_funnel_growth_suite",
    sections=sections,
    regenerate_failed_sections=my_regen_callback,
    draft_mode=True,  # Skip strict failure
)
```

---

### Step 5: Comprehensive Documentation and Copilot Prompt Integration

**Objective**: Document the systematic approach for future similar issues.

**This Document**: `BENCHMARK_FIX_COMPREHENSIVE_GUIDE.md`

**Copilot Prompt Template** for future benchmark issues:

```markdown
## Copilot: Fix Benchmark Validation Failure

**Problem**: [Section] section failing benchmark validation for [Pack]
- Error: [Full error message]
- Failing checks: [List of failing validations]

**5-Step Solution**:

1. **Analyze Benchmark Spec**
   - File: learning/benchmarks/section_benchmarks.[pack].json
   - Extract: word_count, headings, required_headings, bullets, format, forbidden_phrases
   - Document in generator's docstring

2. **Refactor Generator** 
   - Location: backend/main.py, function _gen_[section]()
   - Update to explicitly satisfy all constraints
   - Test each constraint programmatically

3. **Add Logging**
   - Location: backend/validators/report_enforcer.py (lines 120-150)
   - Add structured error logging with extra={}

4. **Create Tests**
   - File: backend/tests/test_[section]_compliance.py
   - Test each constraint: word count, headings, bullets, format, forbidden phrases
   - Run: pytest backend/tests/test_[section]_compliance.py -v

5. **Optional: Enable Draft Mode**
   - Add draft_mode=True parameter for development iteration
   - Skips strict validation but logs warnings

**Success Criteria**:
- All compliance tests pass (green)
- Generator produces output meeting all benchmark constraints
- No forbidden phrases present
- Logging shows clear error summaries if issues occur
```

---

## Quick Reference Checklist

### For New Benchmark Issues

- [ ] Read benchmark spec from `learning/benchmarks/section_benchmarks.*.json`
- [ ] Add explicit constraint documentation to generator docstring
- [ ] Refactor generator code to satisfy all constraints
- [ ] Create test file with constraint verification tests
- [ ] Run tests and verify all pass
- [ ] Check structured logging in report_enforcer.py
- [ ] Optionally enable draft_mode for development
- [ ] Commit and push fix
- [ ] Verify production deployment succeeds

### For Testing Generator Compliance

```bash
cd /workspaces/AICMO

# Quick test
python << 'PYEOF'
from backend.main import GenerateRequest, _gen_[section]
from aicmo.io.client_reports import ClientInputBrief, BrandBrief, ...

brief = ClientInputBrief(...)  # Create test brief
req = GenerateRequest(brief=brief, package_preset="[pack]")
content = _gen_[section](req, pack_key="[pack]")

# Verify metrics
print(f"Words: {len(content.split())} (target: 300-1000)")
print(f"Headings: {content.count('##')} (target: 4-10)")
print(f"Bullets: {content.count('- ')} (target: 12-40)")
PYEOF

# Run full test suite
python -m pytest backend/tests/test_[section]_compliance.py -v
```

---

## Key Lessons Learned

1. **Explicit Constraints**: Add benchmark requirements to generator docstring for clarity
2. **Test-Driven**: Create compliance tests BEFORE making fixes (or alongside)
3. **Logging First**: Log validation errors clearly for faster debugging
4. **Iteration Support**: Draft mode allows development work without strict failures
5. **Specificity Matters**: Use brand/goal/industry language to lower genericness scores
6. **Word Economy**: Remove verbose modifiers and use direct language
7. **Required Headings**: Always verify specific required heading names in spec

---

## Related Files

- **Generator**: `backend/main.py` (line ~5438 for _gen_full_30_day_calendar)
- **Validation Logic**: `backend/validators/report_enforcer.py`
- **Benchmark Spec**: `learning/benchmarks/section_benchmarks.*.json`
- **Tests**: `backend/tests/test_*_compliance.py`
- **Logging**: Configure via Python logging module (see report_enforcer.py)

---

## Example: full_30_day_calendar Fix Summary

| Step | Completed | Details |
|------|-----------|---------|
| 1. Generator | âœ“ | Rewrote _gen_full_30_day_calendar with explicit constraints |
| 2. Logging | âœ“ | Enhanced report_enforcer.py with detailed error summaries |
| 3. Tests | âœ“ | Created test_full_30_day_calendar_compliance.py (9 tests, all pass) |
| 4. Draft Mode | âœ“ | Added draft_mode parameter to enforce_benchmarks_with_regen() |
| 5. Documentation | âœ“ | This comprehensive guide |

**Final Metrics**:
- Word count: 843 (within 300-1000)
- Headings: 6 (within 4-10)
- Bullets: 23 (within 12-40)
- Markdown table: âœ“ (pipe characters present)
- Forbidden phrases: âœ“ (none present)
- Test pass rate: 100% (9/9 tests pass)

---

## Extending This Approach

To fix similar issues in other sections:

1. Identify the failing section and pack
2. Read the benchmark spec for that section
3. Follow the 5-step framework above
4. Create section-specific compliance tests
5. Verify through testing before production deployment

The pattern is consistent across all sections and packs.
