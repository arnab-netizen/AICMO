# All-Packs Simulation Hardening - Complete Implementation

**Date:** 2025-12-05  
**Status:** âœ… COMPLETE  
**Goal:** Make the all-packs simulation system honest, CI-friendly, and never lie about stub usage

---

## Executive Summary

The all-packs simulation system has been **hardened** to:

1. âœ… **Never lie about stub usage** - Stub content is explicitly tracked and reported
2. âœ… **Run as proper pytest suite** - Full parametrization with 20 test cases
3. âœ… **CI-friendly behavior** - Tests skip with clear messages when no LLM configured
4. âœ… **Strict validation when LLM enabled** - All quality gates enforced, stub usage causes failure

### Key Results

**Without LLM configured:**
```
pytest backend/tests/test_all_packs_simulation.py
======================= 20 skipped, 13 warnings in 7.62s =======================
```

**With stub content (current state):**
```
Total: 10
Passed (real LLM): 0
Failed: 7
Stub content used: 10

âš ï¸  STUB CONTENT WARNING:
   Tests using stub content cannot be considered valid passes.
   Configure LLM (OPENAI_API_KEY/ANTHROPIC_API_KEY) for real validation.
```

---

## Changes Made

### 1. Backend: Add `stub_used` Flag to API Response

**File:** `backend/main.py`

**Location 1 - Track stub usage (lines ~7869-7886):**
```python
# Check if LLM should be used
use_llm = os.getenv("AICMO_USE_LLM", "0") == "1"
stub_used = False  # Track whether stub content was used

if use_llm:
    try:
        # Use LLM to generate marketing plan
        marketing_plan = await generate_marketing_plan(req.brief)
        base_output = _generate_stub_output(req)
        # Update with LLM-generated marketing plan
        base_output.marketing_plan = marketing_plan
        stub_used = False  # LLM successfully used
    except Exception as e:
        logger.warning(f"LLM marketing plan generation failed, using stub: {e}", exc_info=False)
        base_output = _generate_stub_output(req)
        stub_used = True  # Fell back to stub
else:
    # Default: offline & deterministic (current behaviour)
    base_output = _generate_stub_output(req)
    stub_used = True  # No LLM configured
```

**Location 2 - Include flag in response (lines ~8620-8625):**
```python
# Phase 3: Build final result
final_result = {
    "report_markdown": report_markdown,
    "status": "success",
    "stub_used": stub_used if "stub_used" in locals() else True,  # Include stub flag
}
```

**Why:** This creates a programmatic signal that test code can use to detect stub usage, eliminating false positives.

---

### 2. Test Suite: Add `stub_used` to PackSimulationResult

**File:** `backend/tests/test_all_packs_simulation.py`

**Change 1 - Dataclass field (lines ~43-56):**
```python
@dataclass
class PackSimulationResult:
    """Hard evidence of pack simulation outcome."""
    pack_key: str
    success: bool
    reason: str
    required_terms_missing: list[str] = field(default_factory=list)
    forbidden_terms_found: list[str] = field(default_factory=list)
    brand_mentions: int = 0
    markdown_length: int = 0
    markdown_excerpt: str = ""
    pdf_rendered: bool = False
    quality_gate_passed: bool = False
    error_message: str = ""
    stub_used: bool = True  # Track whether stub content was used
```

**Change 2 - Extract stub flag from API response (lines ~100-112):**
```python
# Extract results
status = result.get("status", "error")
markdown = result.get("report_markdown", "")
stub_used = result.get("stub_used", True)  # Extract stub flag
markdown_lower = markdown.lower()

if status != "success":
    return PackSimulationResult(
        pack_key=pack_key,
        success=False,
        reason=f"Generation failed: {result.get('error_message', 'Unknown error')}",
        error_message=result.get("error_message", ""),
        markdown_excerpt=markdown[:500] if markdown else "(empty)",
        stub_used=True,
    )
```

**Change 3 - Wire stub_used through success path (lines ~148-162):**
```python
return PackSimulationResult(
    pack_key=pack_key,
    success=success,
    reason=reason,
    required_terms_missing=missing_terms,
    forbidden_terms_found=found_forbidden,
    brand_mentions=brand_mentions,
    markdown_length=len(markdown),
    markdown_excerpt=markdown[:300],
    pdf_rendered=False,
    quality_gate_passed=status == "success",
    stub_used=stub_used,  # Include stub flag
)
```

---

### 3. Test Suite: Convert to Parametrized Pytest

**File:** `backend/tests/test_all_packs_simulation.py`

**Change 1 - Add LLM detection and benchmark list (lines ~1-40):**
```python
import os

# Detect if LLM is configured
LLM_CONFIGURED = bool(os.getenv("OPENAI_API_KEY")) or bool(os.getenv("ANTHROPIC_API_KEY"))

# All benchmark files for parametrization
ALL_BENCHMARK_FILES = [
    "pack_quick_social_basic_d2c.json",
    "pack_strategy_campaign_basic_fitness.json",
    "agency_report_automotive_luxotica.json",
    "pack_strategy_campaign_premium_edtech.json",
    "pack_strategy_campaign_enterprise_financial.json",
    "pack_full_funnel_growth_suite_saas.json",
    "pack_launch_gtm_consumer_electronics.json",
    "pack_brand_turnaround_furniture.json",
    "pack_retention_crm_coffee.json",
    "pack_performance_audit_activegear.json",
]
```

**Change 2 - Main parametrized test with honest behavior (lines ~186-245):**
```python
@pytest.mark.integration
@pytest.mark.parametrize("benchmark_file", ALL_BENCHMARK_FILES)
def test_pack_meets_benchmark(benchmark_file: str):
    """
    Test that a pack meets its benchmark criteria.
    
    Behavior:
    - If LLM not configured: Skip test with clear message
    - If LLM configured but stub used: Fail with "stub content used"
    - If LLM configured and no stub: Validate all quality gates
    """
    # STEP 4: Honest behavior when no LLM configured
    if not LLM_CONFIGURED:
        pytest.skip(
            "LLM not configured (OPENAI_API_KEY/ANTHROPIC_API_KEY missing) â€“ "
            "all-packs simulation cannot validate quality."
        )
    
    # Run the simulation
    result = run_pack_simulation_sync(benchmark_file)
    
    # STEP 5: Honest behavior when LLM configured - strict validation
    
    # Assert 1: Stub content must not be used
    assert not result.stub_used, (
        f"{result.pack_key} used stub content â€“ cannot be considered agency-grade.\n"
        f"This indicates LLM generation failed or was bypassed.\n"
        f"Excerpt: {result.markdown_excerpt[:200]}"
    )
    
    # Assert 2: Required terms must all be present
    assert not result.required_terms_missing, (
        f"{result.pack_key} missing required terms: {result.required_terms_missing}\n"
        f"These domain-specific terms must appear in agency-grade output.\n"
        f"Excerpt: {result.markdown_excerpt[:200]}"
    )
    
    # Assert 3: Forbidden terms must be absent
    assert not result.forbidden_terms_found, (
        f"{result.pack_key} contains forbidden terms: {result.forbidden_terms_found}\n"
        f"These terms indicate wrong industry/domain context.\n"
        f"Excerpt: {result.markdown_excerpt[:200]}"
    )
    
    # Assert 4: Brand mentions must meet minimum threshold
    benchmark = load_pack_benchmark(benchmark_file)
    min_brand_mentions = benchmark.get("min_brand_mentions", 3)
    assert result.brand_mentions >= min_brand_mentions, (
        f"{result.pack_key} has insufficient brand mentions: "
        f"{result.brand_mentions} < {min_brand_mentions}\n"
        f"Agency-grade reports must be brand-focused.\n"
        f"Excerpt: {result.markdown_excerpt[:200]}"
    )
    
    # If all assertions pass, the test succeeds
    print(f"\nâœ… {result.pack_key} PASSED")
    print(f"   Brand mentions: {result.brand_mentions}")
    print(f"   Markdown length: {result.markdown_length}")
    print(f"   Stub used: {result.stub_used}")
```

**Change 3 - Updated `__main__` block with stub tracking (lines ~309-378):**
```python
if __name__ == "__main__":
    """Run all simulations and print summary."""
    print("=" * 80)
    print("AICMO ALL-PACKS SIMULATION - REAL EXECUTION")
    print("=" * 80)
    print()
    
    # Check LLM configuration
    if LLM_CONFIGURED:
        print("âœ… LLM CONFIGURED (tests will validate quality gates)")
    else:
        print("âš ï¸  NO LLM CONFIGURED (tests will be SKIPPED)")
        print("   Set OPENAI_API_KEY or ANTHROPIC_API_KEY to enable LLM validation")
    print()
    
    benchmarks = ALL_BENCHMARK_FILES
    
    results = []
    for benchmark_file in benchmarks:
        print(f"Running: {benchmark_file}...")
        result = run_pack_simulation_sync(benchmark_file)
        results.append(result)
        
        # Determine status based on stub usage and success
        if result.stub_used:
            status = "ðŸ”¶ STUB"
        elif result.success:
            status = "âœ… PASS"
        else:
            status = "âŒ FAIL"
        
        print(f"{status} - {result.pack_key}: {result.reason[:100]}")
        print(f"       Stub used: {result.stub_used}, Brand mentions: {result.brand_mentions}")
        print()
    
    # Summary
    print("=" * 80)
    print("SUMMARY")
    print("=" * 80)
    passed = sum(1 for r in results if r.success and not r.stub_used)
    failed = sum(1 for r in results if not r.success)
    stub_used = sum(1 for r in results if r.stub_used)
    
    print(f"Total: {len(results)}")
    print(f"Passed (real LLM): {passed}")
    print(f"Failed: {failed}")
    print(f"Stub content used: {stub_used}")
    print()
    
    if stub_used > 0:
        print("âš ï¸  STUB CONTENT WARNING:")
        print("   Tests using stub content cannot be considered valid passes.")
        print("   Configure LLM (OPENAI_API_KEY/ANTHROPIC_API_KEY) for real validation.")
        print()
    
    if failed > 0:
        print("FAILURES:")
        for r in results:
            if not r.success:
                print(f"  - {r.pack_key}: {r.reason}")
        print()
    
    if not LLM_CONFIGURED:
        print("âŒ VALIDATION INCOMPLETE: No LLM configured")
        print("   Run pytest to see tests properly skipped.")
    elif stub_used == 0 and failed == 0:
        print("âœ… ALL TESTS PASSED WITH REAL LLM CONTENT")
    else:
        print(f"âš ï¸  {failed + stub_used} of {len(results)} tests need attention")
```

---

## Test Execution Evidence

### Test 1: Pytest with No LLM Configured (Current State)

**Command:**
```bash
cd /workspaces/AICMO
pytest -v backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark
```

**Result:**
```
backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark[pack_quick_social_basic_d2c.json] SKIPPED
backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark[pack_strategy_campaign_basic_fitness.json] SKIPPED
backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark[agency_report_automotive_luxotica.json] SKIPPED
backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark[pack_strategy_campaign_premium_edtech.json] SKIPPED
backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark[pack_strategy_campaign_enterprise_financial.json] SKIPPED
backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark[pack_full_funnel_growth_suite_saas.json] SKIPPED
backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark[pack_launch_gtm_consumer_electronics.json] SKIPPED
backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark[pack_brand_turnaround_furniture.json] SKIPPED
backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark[pack_retention_crm_coffee.json] SKIPPED
backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark[pack_performance_audit_activegear.json] SKIPPED

======================= 10 skipped, 13 warnings in 7.67s =======================
```

**Skip reason:** `"LLM not configured (OPENAI_API_KEY/ANTHROPIC_API_KEY missing) â€“ all-packs simulation cannot validate quality."`

âœ… **HONEST:** Tests don't pretend to pass when no LLM is available

---

### Test 2: All Tests (Parametrized + Legacy)

**Command:**
```bash
cd /workspaces/AICMO
pytest backend/tests/test_all_packs_simulation.py --tb=no -q
```

**Result:**
```
======================= 20 skipped, 13 warnings in 7.62s =======================
```

- 10 parametrized tests (one per benchmark)
- 10 legacy individual test functions (backward compatibility)
- **All 20 skipped** because no LLM configured

---

### Test 3: Direct Python Execution (Stub Tracking)

**Command:**
```bash
cd /workspaces/AICMO
python backend/tests/test_all_packs_simulation.py
```

**Output (selected excerpts):**
```
================================================================================
AICMO ALL-PACKS SIMULATION - REAL EXECUTION
================================================================================

âš ï¸  NO LLM CONFIGURED (tests will be SKIPPED)
   Set OPENAI_API_KEY or ANTHROPIC_API_KEY to enable LLM validation

Running: pack_quick_social_basic_d2c.json...
ðŸ”¶ STUB - quick_social_basic: Forbidden terms found: ['ARR']
       Stub used: True, Brand mentions: 18

Running: pack_strategy_campaign_basic_fitness.json...
ðŸ”¶ STUB - strategy_campaign_basic: Missing required terms: ['fitness']
       Stub used: True, Brand mentions: 16

Running: pack_launch_gtm_consumer_electronics.json...
ðŸ”¶ STUB - launch_gtm_pack: All checks passed
       Stub used: True, Brand mentions: 17

================================================================================
SUMMARY
================================================================================
Total: 10
Passed (real LLM): 0
Failed: 7
Stub content used: 10

âš ï¸  STUB CONTENT WARNING:
   Tests using stub content cannot be considered valid passes.
   Configure LLM (OPENAI_API_KEY/ANTHROPIC_API_KEY) for real validation.

FAILURES:
  - quick_social_basic: Forbidden terms found: ['ARR']
  - strategy_campaign_basic: Missing required terms: ['fitness']
  - unknown: Missing required terms: ['test drive', 'luxury car']; Forbidden: ['ARR']
  - strategy_campaign_premium: Missing required terms: ['bootcamp', 'enrollment']
  - strategy_campaign_enterprise: Missing required terms: ['investment']
  - full_funnel_growth_suite: Missing required terms: ['collaboration', 'lead generation']
  - brand_turnaround_lab: Missing required terms: ['turnaround']

âŒ VALIDATION INCOMPLETE: No LLM configured
   Run pytest to see tests properly skipped.
```

âœ… **HONEST:** 
- Shows `ðŸ”¶ STUB` for all tests
- **Passed (real LLM): 0** explicitly stated
- Clear warning that stub content â‰  valid passes
- Directs user to configure LLM for real validation

---

## Behavior Matrix

| LLM Configured | Stub Used | Result | Message |
|---------------|-----------|--------|---------|
| âŒ No | N/A | **SKIPPED** | "LLM not configured â€“ all-packs simulation cannot validate quality." |
| âœ… Yes | âœ… Yes (fallback) | **FAIL** | "{pack_key} used stub content â€“ cannot be considered agency-grade." |
| âœ… Yes | âŒ No | Validate quality gates | Passes if all gates pass, fails with specific reasons otherwise |

---

## Expected Behavior When LLM Enabled

When `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` is set:

1. Tests will **execute** (not skip)
2. If LLM generation succeeds (`stub_used=False`):
   - Tests validate all quality gates
   - Pass only if ALL criteria met:
     - No stub usage
     - All required terms present
     - No forbidden terms found
     - Brand mentions â‰¥ threshold
3. If LLM generation fails and falls back to stub (`stub_used=True`):
   - Test **FAILS** with: "{pack_key} used stub content â€“ cannot be considered agency-grade."
   - This is **honest** - stub content should not pass agency-grade validation

---

## Quality Gates Enforced (When LLM Enabled)

For each pack, the test validates:

### Assert 1: No Stub Usage
```python
assert not result.stub_used, (
    f"{result.pack_key} used stub content â€“ cannot be considered agency-grade."
)
```

### Assert 2: Required Terms Present
```python
assert not result.required_terms_missing, (
    f"{result.pack_key} missing required terms: {result.required_terms_missing}"
)
```

### Assert 3: Forbidden Terms Absent
```python
assert not result.forbidden_terms_found, (
    f"{result.pack_key} contains forbidden terms: {result.forbidden_terms_found}"
)
```

### Assert 4: Brand Mentions Threshold
```python
assert result.brand_mentions >= min_brand_mentions, (
    f"{result.pack_key} has insufficient brand mentions: {result.brand_mentions} < {min_brand_mentions}"
)
```

---

## CI/CD Integration

### Recommended CI Configuration

```yaml
# .github/workflows/all-packs-test.yml
name: All Packs Simulation

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.12'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Run all-packs simulation
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          # OR: ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          pytest -xvs backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark
```

### Expected CI Outcomes

**Without secrets configured:**
- âœ… 10 tests **skipped** (not failed)
- Clear message: "LLM not configured"
- CI passes (skips are not failures)

**With secrets configured:**
- Tests execute with real LLM
- Failures show exact quality gate violations
- CI fails if any pack doesn't meet benchmarks

---

## Files Modified

1. **backend/main.py**
   - Added `stub_used` flag tracking
   - Wired flag through `api_aicmo_generate_report` return value

2. **backend/tests/test_all_packs_simulation.py**
   - Added `stub_used` field to `PackSimulationResult`
   - Added `LLM_CONFIGURED` detection
   - Created `ALL_BENCHMARK_FILES` list
   - Implemented parametrized `test_pack_meets_benchmark()`
   - Updated all legacy test functions to call parametrized test
   - Enhanced `__main__` block with stub tracking and warnings

---

## Verification Checklist

- [x] Tests skip when no LLM configured (pytest shows "10 skipped")
- [x] Skip message is clear and actionable
- [x] Direct Python execution shows stub usage clearly
- [x] Summary distinguishes "Passed (real LLM)" from "Stub content used"
- [x] All 10 benchmark files tested
- [x] Legacy test functions preserved for backward compatibility
- [x] `stub_used` flag propagates from backend â†’ test result
- [x] CI-friendly: skipped tests don't cause CI failure
- [x] When LLM enabled, stub usage causes test failure (by design)

---

## Next Steps

### To Enable Real Validation

1. **Set API key:**
   ```bash
   export OPENAI_API_KEY="sk-..."
   # OR
   export ANTHROPIC_API_KEY="sk-ant-..."
   ```

2. **Set AICMO to use LLM:**
   ```bash
   export AICMO_USE_LLM="1"
   ```

3. **Run tests:**
   ```bash
   pytest -xvs backend/tests/test_all_packs_simulation.py::test_pack_meets_benchmark
   ```

4. **Expected result:**
   - Tests execute (not skipped)
   - Most/all should pass with real LLM content
   - Any failures show exact quality gate violations

### Future Enhancements

1. **Add section-level validation:**
   - Verify minimum section count per pack
   - Check calendar completeness (30-day coverage)
   - Validate required components (KPI framework, funnel stages)

2. **PDF rendering validation:**
   - Check PDF byte size > 0
   - Verify template binding success
   - Add visual regression tests

3. **Performance benchmarks:**
   - Track generation time per pack
   - Monitor token usage
   - Alert on regression

---

## Conclusion

The all-packs simulation system is now **production-ready** and **honest**:

- âœ… Never reports false positives (stub content clearly marked)
- âœ… CI-friendly (skips without LLM, fails with specific reasons when LLM enabled)
- âœ… Proper pytest integration (parametrized tests, clear assertions)
- âœ… Observable behavior (detailed logging, summary reports)

**No more lies. Only facts.**
